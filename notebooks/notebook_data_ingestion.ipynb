{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c680ba7",
   "metadata": {},
   "source": [
    "# Data & Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b6e85",
   "metadata": {},
   "source": [
    "## Web Scraping --> Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22592b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path('..').resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def scrape_esilv_website():\n",
    "    start_url = \"https://www.esilv.fr/\"\n",
    "    output_dir = \"data/raw/web/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = [start_url]\n",
    "\n",
    "    while to_visit:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        # ❌ Ignorer les pages en anglais\n",
    "        if \"/en/\" in urlparse(url).path:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Nettoyer texte\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            text = soup.get_text(separator=\"\\n\")\n",
    "            text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "            # Nom de fichier basé sur URL, plus court et unique\n",
    "            path_part = urlparse(url).path.replace(\"/\", \"_\").strip(\"_\") or \"home\"\n",
    "            # Tronquer si trop long\n",
    "            max_length = 50\n",
    "            if len(path_part) > max_length:\n",
    "                hash_part = hashlib.md5(path_part.encode()).hexdigest()[:8]\n",
    "                path_part = path_part[:max_length] + \"_\" + hash_part\n",
    "            filename = f\"{path_part}.txt\"\n",
    "\n",
    "            with open(os.path.join(output_dir, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            # Trouver tous les liens internes\n",
    "            for a_tag in soup.find_all(\"a\", href=True):\n",
    "                link = urljoin(start_url, a_tag['href'])\n",
    "\n",
    "                # ❌ Ignorer les liens vers /download/\n",
    "                if link.startswith(\"https://www.esilv.fr/download/\"):\n",
    "                    continue\n",
    "                # ❌ Ignorer les pages en anglais\n",
    "                if \"/en/\" in urlparse(link).path:\n",
    "                    continue\n",
    "                # Ajouter seulement les liens internes\n",
    "                if urlparse(link).netloc == urlparse(start_url).netloc:\n",
    "                    if link not in visited and link not in to_visit:\n",
    "                        to_visit.append(link)\n",
    "\n",
    "            print(f\"[OK] {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Erreur] {url} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_esilv_website()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ecb5a",
   "metadata": {},
   "source": [
    "## Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350bb8c",
   "metadata": {},
   "source": [
    "### Clean up the text\n",
    "\n",
    "Remove:\n",
    "- Multiple spaces\n",
    "- Unnecessary line breaks\n",
    "- Recurring ads/menus\n",
    "- Special characters\n",
    "- Overly short content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2246a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if len(line.strip()) > 20]\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb4585",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "- RAG with between 300 and 600 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=300):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = \" \".join(words[i:i+max_words])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad98714",
   "metadata": {},
   "source": [
    "### Complete pipeline for scraped files\n",
    "- Loads all .txt files from data/raw/web\n",
    "- Cleans\n",
    "- Chops into chunks\n",
    "- Saves to data/processed/web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_dir = \"../data/raw/web/\"\n",
    "processed_dir = \"../data/processed/web/\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(raw_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(raw_dir, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        cleaned = clean_text(raw_text)\n",
    "        chunks = chunk_text(cleaned, max_words=300)\n",
    "\n",
    "        # Sauvegarder chaque chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            out_path = os.path.join(processed_dir, f\"{filename.replace('.txt','')}_chunk{i}.txt\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                out.write(chunk)\n",
    "\n",
    "        print(f\"[OK] {filename} → {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866da33",
   "metadata": {},
   "source": [
    "### PDF Processing (Brochures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "raw_pdf_dir = \"../data/raw/brochures/\"\n",
    "processed_pdf_dir = \"../data/processed/brochures/\"\n",
    "os.makedirs(processed_pdf_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(raw_pdf_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        with pdfplumber.open(os.path.join(raw_pdf_dir, filename)) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        cleaned = clean_text(text)\n",
    "        chunks = chunk_text(cleaned)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            out_path = os.path.join(processed_pdf_dir, f\"{filename.replace('.pdf','')}_chunk{i}.txt\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                out.write(chunk)\n",
    "\n",
    "        print(f\"[OK] {filename} → {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c92dbc",
   "metadata": {},
   "source": [
    "### Combine everything into a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad07a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dir = \"../data/processed/combined/\"\n",
    "os.makedirs(combined_dir, exist_ok=True)\n",
    "\n",
    "for folder in [\"../data/processed/web/\", \"../data/processed/brochures/\"]:\n",
    "    for f in os.listdir(folder):\n",
    "        src = os.path.join(folder, f)\n",
    "        dst = os.path.join(combined_dir, f)\n",
    "        with open(src, \"r\", encoding=\"utf-8\") as infile, open(dst, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977ef1b",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99586bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cassi\\OneDrive\\Documents\\ESILV_A5S1_V2\\LLM and Gen AI\\Projet\\ESILV_Smart_assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1047/1047 [1:27:54<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 66986 embeddings, dim=384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = Path(\"../data/processed/combined/\")\n",
    "model_name = \"all-MiniLM-L6-v2\"  # rapide et bon pour RAG\n",
    "batch_size = 64\n",
    "index_path = Path(\"embeddings/faiss_index.idx\")\n",
    "meta_path = Path(\"embeddings/metadata.json\")\n",
    "\n",
    "files = [p for p in data_dir.glob(\"*.txt\")]\n",
    "texts = []\n",
    "metas = []\n",
    "for i, p in enumerate(sorted(files)):\n",
    "    txt = p.read_text(encoding=\"utf-8\").strip()\n",
    "    if len(txt) < 50:\n",
    "        continue\n",
    "    texts.append(txt)\n",
    "    metas.append({\"id\": i, \"source\": str(p.name)})\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(texts), batch_size)):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "    embeddings.append(emb)\n",
    "embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "os.makedirs(index_path.parent, exist_ok=True)\n",
    "faiss.write_index(index, str(index_path))\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metas, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(texts)} embeddings, dim={dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711d814",
   "metadata": {},
   "source": [
    "### Querying some FAISS indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 score: 0.7864742279052734 source: actuariat-best-job-2015-double-diplome-isup_chunk4.txt\n",
      "2 score: 0.78709876537323 source: bachelor-en-ecole-dingenieurs-quels-stages-pour-qu_0f77f7c5_chunk0.txt\n",
      "3 score: 0.8098337054252625 source: plaquette_apprentissage_esilv_chunk13.txt\n",
      "4 score: 0.8124736547470093 source: lingenieur-dans-le-monde-quels-diplomes-et-niveaux_69fb9fa3_chunk5.txt\n",
      "5 score: 0.8165364265441895 source: jeremie-diplome-2008-en-ingenierie-financiere-est-_890ba857_chunk3.txt\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "index = faiss.read_index(\"embeddings/faiss_index.idx\")\n",
    "with open(\"embeddings/metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metas = json.load(f)\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "q = \"Quelle est la mission de l'école ?\"  # exemple\n",
    "q_emb = model.encode([q], convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "k = 5\n",
    "D, I = index.search(q_emb, k)\n",
    "for rank, idx in enumerate(I[0]):\n",
    "    print(rank+1, \"score:\", float(D[0][rank]), \"source:\", metas[idx][\"source\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
